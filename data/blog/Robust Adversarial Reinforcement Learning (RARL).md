---
title: Robust Adversarial Reinforcement Learning (RARL) 
date: 2023-03-22 
tags: ['RL']
draft: false
authors: ['default']
bibliography: references-data.bib
link-citations: true
---


## What is the issue?
There are two possible ways to perform policy learning for
real-world physical tasks:

1. Real-world Policy Learning: scarcity of data, therefore learned policy overfits the training data and fails to generalize to other scenarios (test generalization issue).
2. Learning in simulation: the environment and physics of the simulator are not exactly the same as the real world. This reality gap often results in unsuccessful transfer if the learned policy isn’t robust to modeling errors (simulation transfer issue)

*What we need is an approach that is significantly more stable/robust in learning policies across different runs and initializations while requiring less data during training.*

## The contribution of the paper:
**Insight** modeling errors can be viewed as extra forces/disturbances in the system

**Main idea of RARL:** RARL suggests modeling uncertainties via an adversarial agent that applies disturbance forces to the system. It does so by introducing the RARL schema which jointly trains a pair of agents, a protagonist and an adversary, where the protagonist learns to fulfil the original task goals while being robust to the disruptions generated by its adversary.
The RARL schema is an alternating procedure. In the first phase, we learn the protagonist’s policy while holding the adversary’s policy fixed. Next, the protagonist’s policy is held constant and the adversary’s policy is learned. This sequence is repeated until convergence.

**Why it should work?** The idea is that if we can learn a policy that is robust to all disturbances, then this policy will be robust to changes in training/test situations; and hence generalize well.

Reference: L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta, “Robust adversarial reinforcement learning,” in *International Conference on Machine Learning*, pp. 2817–2826, PMLR, 2017.